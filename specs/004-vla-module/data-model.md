# Data Model: Module 4 — Vision-Language-Action (VLA)

## Overview
This document defines the key data structures, entities, and relationships for the Vision-Language-Action (VLA) system implemented in Module 4. The data model supports the integration of voice processing, cognitive planning, vision systems, and ROS 2 control for humanoid robots.

## 1. Core Entities

### 1.1 VoiceCommand
**Description**: Natural language input from users that initiates the VLA pipeline, containing semantic intent and contextual information

**Fields**:
- `id`: String (unique identifier)
- `transcript`: String (text transcription of the spoken command)
- `timestamp`: DateTime (when the command was received)
- `confidence`: Float (0.0-1.0, confidence in transcription accuracy)
- `intent`: String (semantic intent extracted from the command)
- `entities`: Array<Object> (extracted entities like objects, locations, actions)
- `context`: Object (environmental context when command was issued)
- `status`: Enum (PENDING, PROCESSING, COMPLETED, FAILED)

**Relationships**:
- One-to-Many with `CognitivePlan` (one command can generate multiple plans)
- One-to-Many with `ExecutionContext` (command execution context)

### 1.2 PerceptionState
**Description**: Current understanding of the environment including object positions, spatial relationships, and sensory data from vision and audio systems

**Fields**:
- `id`: String (unique identifier)
- `timestamp`: DateTime (when state was captured)
- `objects`: Array<Object> (detected objects with positions and properties)
- `spatial_map`: Object (spatial relationships between objects)
- `audio_environment`: Object (audio characteristics of the environment)
- `robot_pose`: Object (current position and orientation of the robot)
- `sensor_data`: Object (raw sensor data from various modalities)

**Relationships**:
- One-to-One with `ExecutionContext` (current state for execution)
- One-to-Many with `CognitivePlan` (plans based on this state)

### 1.3 CognitivePlan
**Description**: High-level sequence of tasks generated by LLM reasoning, representing the decomposition of user commands into executable actions

**Fields**:
- `id`: String (unique identifier)
- `command_id`: String (reference to originating VoiceCommand)
- `tasks`: Array<Object> (sequence of tasks to execute)
- `dependencies`: Array<Object> (task dependencies and constraints)
- `alternatives`: Array<Object> (alternative plans for failure scenarios)
- `estimated_duration`: Integer (estimated time to complete in seconds)
- `confidence`: Float (0.0-1.0, confidence in plan feasibility)
- `status`: Enum (PENDING, APPROVED, EXECUTING, COMPLETED, FAILED)

**Relationships**:
- Many-to-One with `VoiceCommand` (generated from command)
- One-to-Many with `ROS2ActionSequence` (multiple action sequences)
- One-to-One with `PerceptionState` (based on state)

### 1.4 ROS2ActionSequence
**Description**: Low-level commands sent to the humanoid robot through ROS 2 middleware, controlling navigation, manipulation, and other behaviors

**Fields**:
- `id`: String (unique identifier)
- `plan_id`: String (reference to parent CognitivePlan)
- `actions`: Array<Object> (sequence of ROS 2 actions to execute)
- `action_types`: Array<String> (types of actions: navigation, manipulation, etc.)
- `parameters`: Object (parameters for each action)
- `callbacks`: Array<Object> (callbacks for monitoring execution)
- `timeout`: Integer (timeout for the sequence in seconds)
- `status`: Enum (PENDING, EXECUTING, COMPLETED, FAILED, CANCELLED)

**Relationships**:
- Many-to-One with `CognitivePlan` (derived from plan)
- One-to-Many with `ExecutionResult` (execution outcomes)

### 1.5 ExecutionContext
**Description**: Environmental conditions, robot state, and constraints that influence how plans are executed and adapted during runtime

**Fields**:
- `id`: String (unique identifier)
- `perception_state_id`: String (reference to current PerceptionState)
- `robot_state`: Object (current state of the robot)
- `environment_constraints`: Object (obstacles, safety zones, etc.)
- `execution_mode`: Enum (AUTONOMOUS, ASSISTED, SIMULATION)
- `safety_limits`: Object (safety parameters and limits)
- `feedback_channels`: Array<String> (channels for status feedback)

**Relationships**:
- One-to-One with `PerceptionState` (current state)
- One-to-Many with `ROS2ActionSequence` (execution contexts for sequences)

## 2. Supporting Entities

### 2.1 VisionDetection
**Description**: Results from computer vision processing including object detection, pose estimation, and scene understanding

**Fields**:
- `id`: String (unique identifier)
- `object_class`: String (class of detected object)
- `confidence`: Float (0.0-1.0, confidence in detection)
- `position_3d`: Object (3D position in world coordinates)
- `bounding_box`: Object (2D bounding box in image coordinates)
- `pose`: Object (orientation and pose information)
- `timestamp`: DateTime (when detection was made)

**Relationships**:
- Many-to-One with `PerceptionState` (part of perception state)

### 2.2 ExecutionResult
**Description**: Outcome of executing a ROS 2 action sequence including success/failure status and performance metrics

**Fields**:
- `id`: String (unique identifier)
- `action_sequence_id`: String (reference to executed sequence)
- `status`: Enum (SUCCESS, FAILURE, PARTIAL_SUCCESS, TIMEOUT)
- `execution_time`: Integer (actual time taken in seconds)
- `metrics`: Object (performance metrics like accuracy, efficiency)
- `errors`: Array<String> (error messages if any)
- `feedback`: String (human-readable execution feedback)
- `timestamp`: DateTime (when execution completed)

**Relationships**:
- Many-to-One with `ROS2ActionSequence` (result of sequence)

### 2.3 SafetyMonitor
**Description**: Safety system that monitors all VLA operations and prevents unsafe actions

**Fields**:
- `id`: String (unique identifier)
- `active`: Boolean (whether safety monitoring is active)
- `safety_rules`: Array<Object> (safety rules and constraints)
- `violation_thresholds`: Object (thresholds for different safety metrics)
- `override_permissions`: Object (permissions for human override)
- `last_check`: DateTime (when last safety check was performed)
- `status`: Enum (SAFE, WARNING, VIOLATION, OVERRIDDEN)

**Relationships**:
- One-to-Many with `ROS2ActionSequence` (monitors all sequences)

## 3. Data Relationships and Flow

### 3.1 VLA Pipeline Flow
```
VoiceCommand -(transcription & intent extraction)-> PerceptionState -(context awareness)-> CognitivePlan -(task decomposition)-> ROS2ActionSequence -(execution)-> ExecutionResult
```

### 3.2 State Management
- `PerceptionState` is updated continuously through sensor data
- `CognitivePlan` references a specific `PerceptionState` for consistency
- `ExecutionContext` provides environmental context for all operations
- `ExecutionResult` updates the system's understanding of current state

### 3.3 Validation Rules
- VoiceCommand confidence must be > 0.7 for processing
- CognitivePlan must reference a PerceptionState no older than 5 seconds
- ROS2ActionSequence must have timeout values defined
- SafetyMonitor must be active during physical robot execution
- ExecutionResult must be recorded for all action sequences

## 4. State Transitions

### 4.1 VoiceCommand States
PENDING → PROCESSING → (COMPLETED/FAILED) based on transcription and intent extraction success

### 4.2 CognitivePlan States
PENDING → APPROVED → EXECUTING → (COMPLETED/FAILED) based on plan approval and execution success

### 4.3 ROS2ActionSequence States
PENDING → EXECUTING → (COMPLETED/FAILED/CANCELLED) based on action execution outcome

## 5. Indexes and Performance Considerations

### 5.1 Required Indexes
- VoiceCommand.timestamp (for temporal queries)
- CognitivePlan.command_id (for command-plan relationships)
- ExecutionResult.action_sequence_id (for result tracking)
- PerceptionState.timestamp (for state history)

### 5.2 Performance Guidelines
- PerceptionState updates at 10Hz minimum
- VoiceCommand processing within 2 seconds
- CognitivePlan generation within 1 second
- ROS2ActionSequence execution monitoring at 100Hz